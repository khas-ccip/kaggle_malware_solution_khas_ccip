import pandas as pd
import numpy as np
import pickle
import json

# To read directory structure from SETTINGS.json file
with open('./../SETTINGS.json', 'r') as f:
	settings = json.load(f)

training_file_name = '.' + settings['TRAIN_DATA_FINAL_PATH']
val_file_name = '.' + settings['VAL_DATA_FINAL_PATH']

training_df = pd.read_csv(training_file_name, low_memory=False)
val_df = pd.read_csv(val_file_name, low_memory=False) # read final train set and holdout validation sets

training_df.drop(labels=['MachineIdentifier'], axis='columns', inplace=True) # drop MachineIdentifier Column from training_set
val_df.drop(labels=['MachineIdentifier'], axis='columns', inplace=True) # drop MachineIdentifier Column from val_df

#Hyperparameter optimization with Hyperopt library

from hyperopt import STATUS_OK, Trials, fmin, hp, tpe
from sklearn.metrics import roc_auc_score

def score(params):
    print('Training with params: ')
    print(params)
    num_round = int(params['n_estimators'])
    del params['n_estimators']
    dtrain = xgb.DMatrix(training_df.drop(labels=['HasDetections'], axis='columns').values, label=training_df['HasDetections'])
    dvalid = xgb.DMatrix(val_df.drop(labels=['HasDetections'], axis='columns').values, label=val_df['HasDetections'])
    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]
    gbm_model = xgb.train(params, dtrain, num_round,
                          evals=watchlist,
                          verbose_eval=20, early_stopping_rounds=50) # Early stopping criteria to prevent overfitting !!!!
    predictions = gbm_model.predict(dvalid,
                                    ntree_limit=gbm_model.best_iteration + 1)
    score = roc_auc_score(testdf['HasDetections'], predictions)
    print("\tScore {0}\n\n".format(score))
    loss = 1 - score
    return {'loss': loss, 'status': STATUS_OK}


def optimize(random_state=42):
    '''
    This is the optimization function that given a space (space here) of 
    hyperparameters and a scoring function (score here), finds the best hyperparameters.
    '''
    # To learn more about XGBoost parameters, head to this page: 
    # https://github.com/dmlc/xgboost/blob/master/doc/parameter.md
    space = {
        'n_estimators': hp.quniform('n_estimators', 100, 1000, 1),
        'eta': hp.quniform('eta', 0.005, 0.5, 0.025),
        # A problem with max_depth casted to float instead of int with
        # the hp.quniform method.
        'max_depth':  hp.choice('max_depth', np.arange(3, 27, dtype=int)),
        'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),
        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),
        'gamma': hp.quniform('gamma', 0.5, 1, 0.05),
        'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),
        'eval_metric': 'auc',
        'objective': 'binary:logistic',
        'nthread': 20,
        'booster': 'gbtree',#hp.choice('booster', ['gbtree', 'dart']),
        'tree_method': 'exact',
        'silent': 1,
        'seed': random_state
    }
    best = fmin(score, space, algo=tpe.suggest, 
                max_evals=10) # 10 trial takes 22 hours
    return best

import gc
import pickle as pkl

best_param_file = '.' + settings['MODEL_CHECKPOINT_DIR'] + 'best_hyperparams.pkl'

best_hyperparams = optimize() # find best hyparparameters monitoring val_df performance

best_hyperparams['booster'] = 'gbtree'
best_hyperparams['eval_metric'] = 'auc'
best_hyperparams['objective'] = 'binary:logistic'
best_hyperparams['seed'] = 42
best_hyperparams['nthread'] = 20
best_hyperparams['silent'] = 1
best_hyperparams['tree_method'] = 'exact'

with open(best_param_file, 'wb') as f:
	pkl.dump(best_hyperparams, f) # write best hyperparams as pickle file
	
#Model Training with the Best Hyperparams
import xgboost as xgb

dtrain = xgb.DMatrix(training_df.drop(labels=['HasDetections'], axis='columns').values, label=training_df['HasDetections'])
dvalid = xgb.DMatrix(val_df.drop(labels=['HasDetections'], axis='columns').values, label=val_df['HasDetections'])

num_round = int(best_hyperparams['n_estimators'])
del best_hyperparams['n_estimators']

watchlist = [(dtrain, 'train'), (dvalid, 'eval')]
gbm_model = xgb.train(best_hyperparams, dtrain, num_round,
                          evals=watchlist,
                          verbose_eval=20, early_stopping_rounds=50)

del dtrain, dvalid, training_df, val_df
gc.collect()

with open('.' + settings['MODEL_CHECKPOINT_DIR'] + 'best_xgb_model.pkl', 'wb') as f:
	pkl.dump(gbm_model, f)
